{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers haystack langchain python-dotenv openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Returns the length of a word.\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "\n",
    "tools = [get_word_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "\n",
    "llm_with_tools = llm.bind(functions=[format_tool_to_openai_function(t) for t in tools])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful assistant, but bad at calculating lengths of words.\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.format_scratchpad import format_to_openai_function_messages\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_function_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIFunctionsAgentOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.agent import AgentFinish\n",
    "\n",
    "user_input = \"how many letters in the word anticonstitutionnellement?\"\n",
    "intermediate_steps = []\n",
    "while True:\n",
    "    output = agent.invoke(\n",
    "        {\n",
    "            \"input\": user_input,\n",
    "            \"intermediate_steps\": intermediate_steps,\n",
    "        }\n",
    "    )\n",
    "    if isinstance(output, AgentFinish):\n",
    "        final_result = output.return_values[\"output\"]\n",
    "        break\n",
    "    else:\n",
    "        print(f\"TOOL NAME: {output.tool}\")\n",
    "        print(f\"TOOL INPUT: {output.tool_input}\")\n",
    "        tool = {\"get_word_length\": get_word_length}[output.tool]\n",
    "        observation = tool.run(output.tool_input)\n",
    "        intermediate_steps.append((output, observation))\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OS Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.huggingface_endpoint import HuggingFaceEndpoint\n",
    "\n",
    "llm = HuggingFaceEndpoint(endpoint_url='https://mwgkynxou5oy7i3j.us-east-1.aws.endpoints.huggingface.cloud', task='text-generation', model_kwargs={'max_new_tokens':300})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(\"Hello, my name is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pydantic_to_openai_function(\n",
    "    model,\n",
    "    *,\n",
    "    name= None,\n",
    "    description = None,\n",
    "):\n",
    "    \"\"\"Converts a Pydantic model to a function description for the OpenAI API.\"\"\"\n",
    "    schema = model.schema() # dereference_refs(model.schema())\n",
    "    schema.pop(\"definitions\", None)\n",
    "    return {\n",
    "        \"name\": name or schema[\"title\"],\n",
    "        \"description\": description or schema[\"description\"],\n",
    "        \"parameters\": schema,\n",
    "    }\n",
    "\n",
    "\n",
    "def convert_pydantic_to_openai_tool(\n",
    "    model,\n",
    "    *,\n",
    "    name= None,\n",
    "    description = None,\n",
    "):\n",
    "    \"\"\"Converts a Pydantic model to a function description for the OpenAI API.\"\"\"\n",
    "    function = convert_pydantic_to_openai_function(\n",
    "        model, name=name, description=description\n",
    "    )\n",
    "    return {\"type\": \"function\", \"function\": function}\n",
    "    \n",
    "def format_tool_to_huggingface_function(tool):\n",
    "    \"\"\"Format tool into the OpenAI function API.\"\"\"\n",
    "    if tool.args_schema:\n",
    "        return convert_pydantic_to_openai_function(\n",
    "            tool.args_schema, name=tool.name, description=tool.description\n",
    "        )\n",
    "    else:\n",
    "        return {\n",
    "            \"name\": tool.name,\n",
    "            \"description\": tool.description,\n",
    "            \"parameters\": {\n",
    "                # This is a hack to get around the fact that some tools\n",
    "                # do not expose an args_schema, and expect an argument\n",
    "                # which is a string.\n",
    "                # And Open AI does not support an array type for the\n",
    "                # parameters.\n",
    "                \"properties\": {\n",
    "                    \"__arg1\": {\"title\": \"__arg1\", \"type\": \"string\"},\n",
    "                },\n",
    "                \"required\": [\"__arg1\"],\n",
    "                \"type\": \"object\",\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind(functions=[format_tool_to_openai_function(t) for t in tools])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import asyncio\n",
    "import json\n",
    "from json import JSONDecodeError\n",
    "from typing import List, Union\n",
    "\n",
    "from langchain_core.agents import AgentAction, AgentActionMessageLog, AgentFinish\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    ")\n",
    "from langchain_core.outputs import ChatGeneration, Generation\n",
    "from langchain.schema.messages import AIMessage\n",
    "from langchain.agents.agent import AgentOutputParser\n",
    "import re\n",
    "import ast\n",
    "\n",
    "class HuggingFaceFunctionsAgentOutputParser(AgentOutputParser):\n",
    "    \"\"\"Parses a message into agent action/finish.\n",
    "\n",
    "    Is meant to be used with HF models, as it relies on the specific\n",
    "    function_call parameter from OpenAI to convey what tools to use.\n",
    "\n",
    "    If a function_call parameter is passed, then that is used to get\n",
    "    the tool and tool input.\n",
    "\n",
    "    If one is not passed, then the AIMessage is assumed to be the final output.\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def _type(self) -> str:\n",
    "        return \"openai-functions-agent\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_ai_message(message: str) -> Union[AgentAction, AgentFinish]:\n",
    "        \"\"\"Parse an AI message. Like:\n",
    "        <functioncall> {\"name\":\"generate_anagram\", \"arguments\": {\"word\": \"anticonstitutionnellement\"}}\n",
    "    \n",
    "        <functionresp>\n",
    "        \"\"\"\n",
    "        # if not isinstance(message, AIMessage):\n",
    "        #     raise TypeError(f\"Expected an AI message got {type(message)}\")\n",
    "\n",
    "        function_call = '<functioncall>' in message\n",
    "\n",
    "        if function_call:\n",
    "            message = message.replace('\\n', '')\n",
    "            marker_begin, marker_end = '<functioncall>', '<functionresp>'\n",
    "            begin = message.find(marker_begin) + len(marker_begin)\n",
    "            end = message.find(marker_end)\n",
    "            if end == -1:\n",
    "                call_text = message[begin:]\n",
    "            else:\n",
    "                call_text = message[begin:end]\n",
    "            call_text = call_text.strip()\n",
    "\n",
    "            function_call = ast.literal_eval(call_text)\n",
    "            function_name = function_call[\"name\"]\n",
    "            _tool_input = function_call[\"arguments\"]\n",
    "\n",
    "            content_msg = f\"responded: {message}\\n\" if message else \"\\n\"\n",
    "            log = f\"\\nInvoking: `{function_name}` with `{tool_input}`\\n{content_msg}\\n\"\n",
    "            return AgentActionMessageLog(\n",
    "                tool=function_name,\n",
    "                tool_input=tool_input,\n",
    "                log=log,\n",
    "                message_log=[AIMessage(content=message)],\n",
    "            )\n",
    "\n",
    "        return AgentFinish(\n",
    "            return_values={\"output\": message}, log=str(message)\n",
    "        )\n",
    "\n",
    "    def parse_result(\n",
    "        self, result: List[Generation], *, partial: bool = False\n",
    "    ) -> Union[AgentAction, AgentFinish]:\n",
    "        # if not isinstance(result[0], ChatGeneration):\n",
    "        #     raise ValueError(\"This output parser only works on ChatGeneration output\")\n",
    "        message = result[0].text\n",
    "        return self._parse_ai_message(message)\n",
    "\n",
    "    async def aparse_result(\n",
    "        self, result: List[Generation], *, partial: bool = False\n",
    "    ) -> Union[AgentAction, AgentFinish]:\n",
    "        return await asyncio.get_running_loop().run_in_executor(\n",
    "            None, self.parse_result, result\n",
    "        )\n",
    "\n",
    "    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\n",
    "        raise ValueError(\"Can only parse messages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_with_calls = \"\"\"<|im_start|>system\n",
    "    You are an helpful assistant who has access to the following functions to help the user, you can use the functions if needed- { \"name\": \"generate_anagram\", \"description\": \"Generate an anagram of a given word\", \"parameters\": { \"type\": \"object\", \"properties\": { \"word\": { \"type\": \"string\", \"description\": \"The word to generate an anagram of\" } }, \"required\": [ \"word\" ] } }\n",
    "   <|im_end|>\n",
    "   <|im_start|>user\n",
    "   Can you help me generate an anagram of the word \"listen\"?\n",
    "   <|im_end|>\n",
    "   <|im_start|>assistant\n",
    "   <functioncall> {\"name\":\"generate_anagram\", \"arguments\": {\"word\": \"listen\"}}\n",
    "   <|im_end|>\n",
    "    <functionresp> {\"anagram\": \"silent\"}\n",
    "    <|im_end|>\n",
    "    <|im_start|>user\n",
    "   Can you help me get the current day as a string?\n",
    "   <|im_end|>\n",
    "   <|im_start|>assistant\n",
    "   <functioncall> {\"name\":\"get_current_day_str\", \"arguments\": {}}\n",
    "   <|im_end|>\n",
    "    <functionresp> {\"str\": \"Monday\"}\n",
    "    <|im_end|>\n",
    "    <|im_start|>user\n",
    "    {{input}}.\n",
    "    <|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    text_with_calls, template_format=\"jinja2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_function_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | HuggingFaceFunctionsAgentOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provisory:\n",
    "- intermediate_steps replaced with a string since I don't know how to put MessagesPlaceholder inside a PromptTemplate\n",
    "- message handled as a string in the InputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_test = \"anticonstitutionnellement\"\n",
    "print(len(word_to_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.agent import AgentFinish\n",
    "\n",
    "intermediate_steps = []\n",
    "history=\"\"\n",
    "while True:\n",
    "    output = agent.invoke(\n",
    "        {\n",
    "            \"input\": f\"Can you get the length of word {word_to_test}? Answer with a function call if needed. For the record, {history}\",\n",
    "            \"intermediate_steps\": intermediate_steps,\n",
    "        }\n",
    "    )\n",
    "    if isinstance(output, AgentFinish):\n",
    "        final_result = output.return_values[\"output\"]\n",
    "        break\n",
    "    else:\n",
    "        tool = {\"get_word_length\": get_word_length}[output.tool]\n",
    "        observation = tool.run(output.tool_input)\n",
    "        history += f\"Tool {output.tool} was called with input {output.tool_input} and returned {observation}\\n\"\n",
    "        intermediate_steps.append((output, observation))\n",
    "print(\"Final answer:\", final_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
