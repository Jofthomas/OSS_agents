{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Returns the length of a word.\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "\n",
    "tools = [get_word_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "\n",
    "llm_with_tools = llm.bind(functions=[format_tool_to_openai_function(t) for t in tools])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful assistant, but bad at calculating lengths of words.\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.format_scratchpad import format_to_openai_function_messages\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_function_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIFunctionsAgentOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.agent import AgentFinish\n",
    "\n",
    "user_input = \"how many letters in the word educa?\"\n",
    "intermediate_steps = []\n",
    "while True:\n",
    "    output = agent.invoke(\n",
    "        {\n",
    "            \"input\": user_input,\n",
    "            \"intermediate_steps\": intermediate_steps,\n",
    "        }\n",
    "    )\n",
    "    if isinstance(output, AgentFinish):\n",
    "        final_result = output.return_values[\"output\"]\n",
    "        break\n",
    "    else:\n",
    "        print(f\"TOOL NAME: {output.tool}\")\n",
    "        print(f\"TOOL INPUT: {output.tool_input}\")\n",
    "        tool = {\"get_word_length\": get_word_length}[output.tool]\n",
    "        observation = tool.run(output.tool_input)\n",
    "        intermediate_steps.append((output, observation))\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OS Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.huggingface_endpoint import HuggingFaceEndpoint\n",
    "\n",
    "llm = HuggingFaceEndpoint(endpoint_url='https://mwgkynxou5oy7i3j.us-east-1.aws.endpoints.huggingface.cloud', task='text-generation', model_kwargs={'max_new_tokens':200})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(\"Hello, my name is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pydantic_to_openai_function(\n",
    "    model,\n",
    "    *,\n",
    "    name= None,\n",
    "    description = None,\n",
    "):\n",
    "    \"\"\"Converts a Pydantic model to a function description for the OpenAI API.\"\"\"\n",
    "    schema = model.schema() # dereference_refs(model.schema())\n",
    "    schema.pop(\"definitions\", None)\n",
    "    return {\n",
    "        \"name\": name or schema[\"title\"],\n",
    "        \"description\": description or schema[\"description\"],\n",
    "        \"parameters\": schema,\n",
    "    }\n",
    "\n",
    "\n",
    "def convert_pydantic_to_openai_tool(\n",
    "    model,\n",
    "    *,\n",
    "    name= None,\n",
    "    description = None,\n",
    "):\n",
    "    \"\"\"Converts a Pydantic model to a function description for the OpenAI API.\"\"\"\n",
    "    function = convert_pydantic_to_openai_function(\n",
    "        model, name=name, description=description\n",
    "    )\n",
    "    return {\"type\": \"function\", \"function\": function}\n",
    "    \n",
    "def format_tool_to_huggingface_function(tool):\n",
    "    \"\"\"Format tool into the OpenAI function API.\"\"\"\n",
    "    if tool.args_schema:\n",
    "        return convert_pydantic_to_openai_function(\n",
    "            tool.args_schema, name=tool.name, description=tool.description\n",
    "        )\n",
    "    else:\n",
    "        return {\n",
    "            \"name\": tool.name,\n",
    "            \"description\": tool.description,\n",
    "            \"parameters\": {\n",
    "                # This is a hack to get around the fact that some tools\n",
    "                # do not expose an args_schema, and expect an argument\n",
    "                # which is a string.\n",
    "                # And Open AI does not support an array type for the\n",
    "                # parameters.\n",
    "                \"properties\": {\n",
    "                    \"__arg1\": {\"title\": \"__arg1\", \"type\": \"string\"},\n",
    "                },\n",
    "                \"required\": [\"__arg1\"],\n",
    "                \"type\": \"object\",\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind(functions=[format_tool_to_openai_function(t) for t in tools])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import asyncio\n",
    "import json\n",
    "from json import JSONDecodeError\n",
    "from typing import List, Union\n",
    "\n",
    "from langchain_core.agents import AgentAction, AgentActionMessageLog, AgentFinish\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    ")\n",
    "from langchain_core.outputs import ChatGeneration, Generation\n",
    "\n",
    "from langchain.agents.agent import AgentOutputParser\n",
    "\n",
    "class HuggingFaceFunctionsAgentOutputParser(AgentOutputParser):\n",
    "    \"\"\"Parses a message into agent action/finish.\n",
    "\n",
    "    Is meant to be used with HF models, as it relies on the specific\n",
    "    function_call parameter from OpenAI to convey what tools to use.\n",
    "\n",
    "    If a function_call parameter is passed, then that is used to get\n",
    "    the tool and tool input.\n",
    "\n",
    "    If one is not passed, then the AIMessage is assumed to be the final output.\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def _type(self) -> str:\n",
    "        return \"openai-functions-agent\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_ai_message(message: BaseMessage) -> Union[AgentAction, AgentFinish]:\n",
    "        \"\"\"Parse an AI message.\"\"\"\n",
    "        print(message)\n",
    "        if not isinstance(message, AIMessage):\n",
    "            raise TypeError(f\"Expected an AI message got {type(message)}\")\n",
    "\n",
    "        function_call = message.additional_kwargs.get(\"function_call\", {})\n",
    "\n",
    "        if function_call:\n",
    "            function_name = function_call[\"name\"]\n",
    "            try:\n",
    "                if len(function_call[\"arguments\"].strip()) == 0:\n",
    "                    # OpenAI returns an empty string for functions containing no args\n",
    "                    _tool_input = {}\n",
    "                else:\n",
    "                    # otherwise it returns a json object\n",
    "                    _tool_input = json.loads(function_call[\"arguments\"])\n",
    "            except JSONDecodeError:\n",
    "                raise OutputParserException(\n",
    "                    f\"Could not parse tool input: {function_call} because \"\n",
    "                    f\"the `arguments` is not valid JSON.\"\n",
    "                )\n",
    "\n",
    "            # HACK HACK HACK:\n",
    "            # The code that encodes tool input into Open AI uses a special variable\n",
    "            # name called `__arg1` to handle old style tools that do not expose a\n",
    "            # schema and expect a single string argument as an input.\n",
    "            # We unpack the argument here if it exists.\n",
    "            # Open AI does not support passing in a JSON array as an argument.\n",
    "            if \"__arg1\" in _tool_input:\n",
    "                tool_input = _tool_input[\"__arg1\"]\n",
    "            else:\n",
    "                tool_input = _tool_input\n",
    "\n",
    "            content_msg = f\"responded: {message.content}\\n\" if message.content else \"\\n\"\n",
    "            log = f\"\\nInvoking: `{function_name}` with `{tool_input}`\\n{content_msg}\\n\"\n",
    "            return AgentActionMessageLog(\n",
    "                tool=function_name,\n",
    "                tool_input=tool_input,\n",
    "                log=log,\n",
    "                message_log=[message],\n",
    "            )\n",
    "\n",
    "        return AgentFinish(\n",
    "            return_values={\"output\": message.content}, log=str(message.content)\n",
    "        )\n",
    "\n",
    "    def parse_result(\n",
    "        self, result: List[Generation], *, partial: bool = False\n",
    "    ) -> Union[AgentAction, AgentFinish]:\n",
    "        print(result)\n",
    "        if not isinstance(result[0], ChatGeneration):\n",
    "            raise ValueError(\"This output parser only works on ChatGeneration output\")\n",
    "        message = result[0].message\n",
    "        return self._parse_ai_message(message)\n",
    "\n",
    "    async def aparse_result(\n",
    "        self, result: List[Generation], *, partial: bool = False\n",
    "    ) -> Union[AgentAction, AgentFinish]:\n",
    "        return await asyncio.get_running_loop().run_in_executor(\n",
    "            None, self.parse_result, result\n",
    "        )\n",
    "\n",
    "    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\n",
    "        raise ValueError(\"Can only parse messages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"text\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"<|im_start|>system\n",
    "    You are an helpful assistant who has access to the following functions to help the user, you can use the functions if needed- { \"name\": \"generate_anagram\", \"description\": \"Generate an anagram of a given word\", \"parameters\": { \"type\": \"object\", \"properties\": { \"word\": { \"type\": \"string\", \"description\": \"The word to generate an anagram of\" } }, \"required\": [ \"word\" ] } }\n",
    "   <|im_end|>\n",
    "   <|im_start|>user\n",
    "   Can you help me generate an anagram of the word \"listen\"?\n",
    "   <|im_end|>\n",
    "   <|im_start|>assistant\n",
    "   <functioncall> {\"name\":\"generate_anagram\", \"arguments\": {\"word\": \"listen\"}}\n",
    "   <|im_end|>\n",
    "    <functionresp> {\"anagram\": \"silent\"}\n",
    "    <|im_end|>\n",
    "   The anagram of the word \"listen\" is \"silent\".\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "That's amazing! Can you generate an anagram for the word \"race\"?\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_function_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | HuggingFaceFunctionsAgentOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(text, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_test = \"anticonstitutionnellement\"\n",
    "print(len(word_to_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.agent import AgentFinish\n",
    "\n",
    "user_input = f\"Can you count thenumber of letters in the word {word_to_test}?\"\n",
    "intermediate_steps = []\n",
    "while True:\n",
    "    output = agent.invoke(\n",
    "        {\n",
    "            \"input\": user_input,\n",
    "            \"intermediate_steps\": intermediate_steps,\n",
    "        }\n",
    "    )\n",
    "    print(output)\n",
    "    if isinstance(output, AgentFinish):\n",
    "        final_result = output.return_values[\"output\"]\n",
    "        break\n",
    "    else:\n",
    "        print(f\"TOOL NAME: {output.tool}\")\n",
    "        print(f\"TOOL INPUT: {output.tool_input}\")\n",
    "        tool = {\"get_word_length\": get_word_length}[output.tool]\n",
    "        observation = tool.run(output.tool_input)\n",
    "        intermediate_steps.append((output, observation))\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
