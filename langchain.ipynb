{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers langchain python-dotenv openai google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents.format_scratchpad import format_to_openai_function_messages\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Returns the length of a word.\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_text_word_count(sentence: str) -> int:\n",
    "    \"\"\"Returns the number of words in a sentence.\"\"\"\n",
    "    return len(sentence.split())\n",
    "\n",
    "\n",
    "wiki_search = GoogleSearchAPIWrapper()\n",
    "\n",
    "@tool\n",
    "def search_wikipedia(query: str) -> str:\n",
    "    \"\"\"Searches Wikipedia for a query.\"\"\"\n",
    "    return wiki_search.run(query)\n",
    "\n",
    "toolbox = {\n",
    "    \"get_word_length\": get_word_length,\n",
    "    \"get_text_word_count\": get_text_word_count,\n",
    "    \"search_wikipedia\": search_wikipedia,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind(functions=[format_tool_to_openai_function(t) for t in toolbox.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful assistant, but bad at calculating lengths of words.\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_function_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | system_prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIFunctionsAgentOutputParser()\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=list(toolbox.values()), verbose=True)\n",
    "\n",
    "# QUESTION = \"Can you count the number of letters in the word anticonstitutionnellement?\"\n",
    "QUESTION = \"Can you get the name of the COP28 host country?\"\n",
    "\n",
    "agent_executor.invoke({\"input\": QUESTION})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OS Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.huggingface_endpoint import HuggingFaceEndpoint\n",
    "\n",
    "endpoint_hermes = 'https://mwgkynxou5oy7i3j.us-east-1.aws.endpoints.huggingface.cloud'\n",
    "\n",
    "llm = HuggingFaceEndpoint(endpoint_url=endpoint_hermes, task='text-generation', model_kwargs={\"do_sample\" : False, \"max_new_tokens\" : 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import asyncio\n",
    "from typing import List, Union\n",
    "\n",
    "from langchain_core.agents import AgentAction, AgentActionMessageLog, AgentFinish\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    ")\n",
    "from langchain_core.outputs import ChatGeneration, Generation\n",
    "from langchain.schema.messages import AIMessage\n",
    "from langchain.agents.agent import AgentOutputParser\n",
    "import ast\n",
    "\n",
    "def extract_function_call_hermes(call: str):\n",
    "    \"\"\"Extracts function name and arguments from a text function call\"\"\"\n",
    "    function_call = ast.literal_eval(call)\n",
    "    return function_call[\"name\"], function_call[\"arguments\"]\n",
    "\n",
    "\n",
    "class HuggingFaceFunctionsAgentOutputParser(AgentOutputParser):\n",
    "    \"\"\"Parses a message into agent action/finish.\n",
    "\n",
    "    Is meant to be used with HF models.\n",
    "    \"\"\"\n",
    "    @property\n",
    "    def _type(self) -> str:\n",
    "        return \"huggingface-functions-agent\"\n",
    "\n",
    "    def parse(self, message: str) -> Union[AgentAction, AgentFinish]:\n",
    "        \"\"\"\n",
    "        Parse a message into function call or final output\n",
    "        \"\"\"\n",
    "        function_call_begin_marker =\"<functioncall>\"\n",
    "        function_call_end_marker =\"<functionresp>\"\n",
    "\n",
    "        function_call = function_call_begin_marker in message\n",
    "\n",
    "        if function_call:\n",
    "            message = message.replace('\\n', '')\n",
    "            marker_begin, marker_end = function_call_begin_marker, function_call_end_marker\n",
    "            begin = message.find(marker_begin) + len(marker_begin)\n",
    "            # end = message.find(marker_end)\n",
    "            end=-1\n",
    "            if end == -1:\n",
    "                end = message.find('<|im_end|>')\n",
    "\n",
    "            if end == -1:\n",
    "                end = message.find('}}') + 2\n",
    "            # If no end was found\n",
    "            if end == -1:\n",
    "                call_text = message[begin:]\n",
    "            else:\n",
    "                call_text = message[begin:end]\n",
    "            call_text = call_text.strip()\n",
    "\n",
    "            function_name, tool_input = extract_function_call_hermes(call_text)\n",
    "            \n",
    "            content_msg = f\"responded: {message}\\n\" if message else \"\\n\"\n",
    "            log = f\"\\nInvoking: `{function_name}` with `{tool_input}`\\n{content_msg}\\n\"\n",
    "            return AgentActionMessageLog(\n",
    "                tool=function_name,\n",
    "                tool_input=tool_input,\n",
    "                log=log,\n",
    "                message_log=[AIMessage(content=message)],\n",
    "            )\n",
    "\n",
    "        return AgentFinish(\n",
    "            return_values={\"output\": message}, log=str(message)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provisory:\n",
    "- message handled as a string in the InputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from prompt_formats import prompt_hermes\n",
    "\n",
    "system_prompt = prompt_hermes\n",
    "\n",
    "system_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            system_prompt,\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_descriptions = [format_tool_to_openai_function(t) for t in toolbox.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind(functions=tool_descriptions)\n",
    "\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"]\n",
    "    }\n",
    "    | system_prompt\n",
    "    | llm_with_tools\n",
    "    | HuggingFaceFunctionsAgentOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.agent import AgentFinish\n",
    "\n",
    "# QUESTION = \"Can you count the letters in word 'bonjourmonami'?\" \n",
    "# QUESTION = \"Can you extend the sentence 'Hello Sir, the weather is terrible' to 10 words? Check that your output sentence has exactly 10 words!\"\n",
    "# QUESTION = \"Can you get the number of words in the sentence 'Hello Sir, terrible weather'?\"\n",
    "QUESTION = \"Can you get the name of the COP28 host country? Only answer with the country name.\"\n",
    "# QUESTION = \"Who are the pitchers with the number before and after Taish≈ç Tamai's number as of July 2023? Give them to me in the form Pitcher Before, Pitcher After, use their last names only, in Roman characters.\"\n",
    "\n",
    "intermediate_steps = []\n",
    "history=''\n",
    "while True:\n",
    "    output = agent.invoke(\n",
    "        {\n",
    "            \"input\": QUESTION ss+ f\" Available functions are {tool_descriptions}. For the record, {history}. Call a function only if needed.\",\n",
    "            \"intermediate_steps\": intermediate_steps,\n",
    "        }\n",
    "    )\n",
    "    if isinstance(output, AgentFinish):\n",
    "        final_result = output.return_values[\"output\"]\n",
    "        break\n",
    "    else:\n",
    "        tool = toolbox[output.tool]\n",
    "        observation = tool.run(output.tool_input)\n",
    "        print(f\"Calling {output.tool} with input {output.tool_input} returned: {observation}\")\n",
    "        history+=f\"calling {output.tool} with input {output.tool_input} returned: {observation} ;\"\n",
    "print(\"AGENT ANSWER:::\", final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
